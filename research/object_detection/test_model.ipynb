{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from pdb import set_trace\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as ET  \n",
    "import cv2\n",
    "\n",
    "\n",
    "from detector.Detector import Detector \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VyPz_t8WWKMQ"
   },
   "outputs": [],
   "source": [
    "FROZEN_GRAPH_PATH = 'C:/Users/iApp/Desktop/projects/object-detection/models/research/object_detection/faster_rcnn_resnet50_coco_2018_01_28/frozen_inference_graph.pb'\n",
    "LABEL_MAP_PATH = 'C:/Users/iApp/Desktop/projects/object-detection/models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "TEST_IMAGES_DIR_PATH =  'C:/Users/iApp/Desktop/projects/lp-ocr-preprocessing/cropped_lps/gs-testset/'\n",
    "TEST_RESULT_DIR = None\n",
    "TESTSET_RATIO = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0_1AGhrWKMc"
   },
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_detections_left_to_right(single_image_detections):\n",
    "    return sorted(single_image_detections, key=lambda x: x['box'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Detection Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3a5wMHN8WKMh"
   },
   "outputs": [],
   "source": [
    "if TESTSET_RATIO > 1:\n",
    "    TESTSET_RATIO = 1\n",
    "elif TESTSET_RATIO < 0:\n",
    "    TESTSET_RATIO = 0\n",
    "    \n",
    "TEST_IMAGE_PATHS =  [f for f in glob.glob(TEST_IMAGES_DIR_PATH + '*.jpg')]\n",
    "n_testset = int(len(TEST_IMAGE_PATHS) * TESTSET_RATIO)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(TEST_IMAGE_PATHS)\n",
    "TEST_IMAGE_PATHS = TEST_IMAGE_PATHS[:n_testset]\n",
    "\n",
    "#TEST_IMAGE_PATHS = TEST_IMAGE_PATHS[:1]\n",
    "\n",
    "all_images_detections = {}\n",
    "with Detector(FROZEN_GRAPH_PATH, LABEL_MAP_PATH) as detector:\n",
    "    for image_path in TEST_IMAGE_PATHS: \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # Sort detections left-to-right\n",
    "        single_image_detections = sort_detections_left_to_right(detector.detect(image, maxDetection=8))\n",
    "\n",
    "        # Validate detection before add\n",
    "\n",
    "\n",
    "        # Append to detections list\n",
    "        image_name = os.path.basename(image_path).split('.')[0]\n",
    "        all_images_detections[image_name] = single_image_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Ground Truth of all images\n",
    "all_images_ground_truths = {}\n",
    "\n",
    "for image_name, _ in all_images_detections.items():\n",
    "    \n",
    "    xml_path = os.path.join(TEST_IMAGES_DIR_PATH, image_name + '.xml')\n",
    "    \n",
    "    single_image_ground_truths = []\n",
    "    \n",
    "    tree = ET.parse(xml_path)\n",
    "    object_elem_list = tree.getroot().findall('object')\n",
    "    for object_elem in object_elem_list:    \n",
    "        ground_truth = {}\n",
    "        bndbox_elem = object_elem.find('bndbox')\n",
    "\n",
    "        xmin = int(float(bndbox_elem.find('xmin').text))\n",
    "        ymin = int(float(bndbox_elem.find('ymin').text))\n",
    "        xmax = int(float(bndbox_elem.find('xmax').text))\n",
    "        ymax = int(float(bndbox_elem.find('ymax').text))\n",
    "\n",
    "        name = object_elem.find('name').text\n",
    "        \n",
    "        ground_truth['box'] = (xmin, ymin, xmax, ymax)\n",
    "        ground_truth['label'] = name\n",
    "        ground_truth['score'] = None\n",
    "\n",
    "        single_image_ground_truths.append(ground_truth) \n",
    "    single_image_ground_truths = sort_detections_left_to_right(single_image_ground_truths)    \n",
    "    all_images_ground_truths[image_name] = single_image_ground_truths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate exact matched recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model can recalls          0.0% of images\n"
     ]
    }
   ],
   "source": [
    "def recall_of_all_images(all_images_ground_truths, all_images_detections):\n",
    "    count = 0\n",
    "    for image_name, single_image_ground_truths in all_images_ground_truths.items():\n",
    "        gt_label = list(map(lambda gt: gt['label'], single_image_ground_truths))\n",
    "        det_label = list(map(lambda det: det['label'], all_images_detections[image_name]))\n",
    "        count += (gt_label == det_label)\n",
    "\n",
    "    return count / len(all_images_ground_truths)\n",
    "all_images_recall = recall_of_all_images(all_images_ground_truths, all_images_detections)\n",
    "print(f\"Model can recalls          {all_images_recall*100:.1f}% of images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TP, FP and FN for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_ground_truths(all_images_ground_truths):\n",
    "    return sum([len(single_image_ground_truths) for _, single_image_ground_truths in all_images_ground_truths.items()])\n",
    "\n",
    "def cal_iou(det_box, gt_box):\n",
    "    intersect_xmin = max([det_box[0], gt_box[0]])\n",
    "    intersect_ymin = max([det_box[1], gt_box[1]])\n",
    "    intersect_xmax = min([det_box[2], gt_box[2]])\n",
    "    intersect_ymax = min([det_box[3], gt_box[3]])\n",
    "    \n",
    "    intersect_area = max(0, intersect_xmax - intersect_xmin + 1) * max(0, intersect_ymax - intersect_ymin + 1)\n",
    "    \n",
    "    det_box_area = (det_box[2] - det_box[0] + 1) * (det_box[3] - det_box[1] + 1)\n",
    "    gt_box_area = (gt_box[2] - gt_box[0] + 1) * (gt_box[3] - gt_box[1] + 1)\n",
    "    \n",
    "    iou = intersect_area / float(det_box_area + gt_box_area - intersect_area)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_threshold = 0.5\n",
    "\n",
    "for image_name, single_image_detections in all_images_detections.items():\n",
    "    single_image_ground_truths = all_images_ground_truths[image_name].copy()\n",
    "    \n",
    "    for det in single_image_detections:\n",
    "        not_found = True\n",
    "        for gt in single_image_ground_truths:\n",
    "            iou = cal_iou(det['box'], gt['box'])\n",
    "            if iou >= iou_threshold:\n",
    "                det['actual'] = gt['label']\n",
    "                not_found = False\n",
    "                break  \n",
    "                \n",
    "        if not_found:        \n",
    "            det['actual'] = 'NA'     \n",
    "\n",
    "for image_name, single_image_ground_truths in all_images_ground_truths.items():\n",
    "    single_image_detection = all_images_detections[image_name].copy()\n",
    "    \n",
    "    for gt in single_image_ground_truths:\n",
    "        not_found = True\n",
    "        for det in single_image_detection:\n",
    "            iou = cal_iou(gt['box'], det['box'])\n",
    "            if iou >= iou_threshold:\n",
    "                gt['predict'] = det['label']\n",
    "                not_found = False\n",
    "                break  \n",
    "                \n",
    "        if not_found:        \n",
    "            gt['predict'] = 'NA'                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP, FP and FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct_detection = sum([det['label'] == det['actual'] for _, single_image_detections in all_images_detections.items() for det in single_image_detections])\n",
    "n_wrong_detection = sum([det['label'] != det['actual'] for _, single_image_detections in all_images_detections.items() for det in single_image_detections])\n",
    "n_undetected = sum([gt['label'] != gt['predict'] for _, single_image_ground_truths in all_images_ground_truths.items() for gt in single_image_ground_truths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All classes Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model correctly recalls about       0.0% of chars\n"
     ]
    }
   ],
   "source": [
    "recall_score = n_correct_detection / total_ground_truths(all_images_ground_truths)\n",
    "print(f\"Model correctly recalls about       {recall_score*100:.1f}% of chars\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All classes Precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model True Positive Rate(Precision) 0.0% of chars\n",
      "Model False Positive Rate           100.0% of chars\n"
     ]
    }
   ],
   "source": [
    "true_positive_rate = n_correct_detection / (n_correct_detection + n_wrong_detection)\n",
    "false_positive_rate = n_wrong_detection / (n_correct_detection + n_wrong_detection)\n",
    "print(f\"Model True Positive Rate(Precision) {true_positive_rate*100:.1f}% of chars\")\n",
    "print(f\"Model False Positive Rate           {false_positive_rate*100:.1f}% of chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "gts_dets = np.array([[gt['label'], gt['predict']] for _, single_image_ground_truths in all_images_ground_truths.items() for gt in single_image_ground_truths])\n",
    "\n",
    "labels = set(gts_dets[:, 0])\n",
    "labels.update(gts_dets[:, 1])\n",
    "labels = sorted(list(labels))\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "\n",
    "conf_mx = confusion_matrix(gts_dets[:, 0], \n",
    "                           gts_dets[:, 1], \n",
    "                           labels=labels)\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(norm_conf_mx, cmap='gray')\n",
    "plt.xticks(np.arange(len(labels)), labels=labels, family='Tahoma')\n",
    "plt.yticks(np.arange(len(labels)), labels=labels, family='Tahoma')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_detection(out_dir, image_path, detections, font, display_label=True):\n",
    "    image_name = os.path.basename(image_path).split('.')[0]\n",
    "    \n",
    "    cleaned_image = Image.open(image_path)\n",
    "    if cleaned_image.mode != 'RGB':\n",
    "        cleaned_image = cleaned_image.convert('RGB')\n",
    "    cleaned_image_drawer = ImageDraw.Draw(cleaned_image)    \n",
    "    \n",
    "    for det in detections:\n",
    "        label = det['label']\n",
    "        xmin, ymin, xmax, ymax = det['box']\n",
    "        cleaned_image_drawer.line([(xmin, ymin), \n",
    "                                   (xmax, ymin),\n",
    "                                   (xmax, ymax),\n",
    "                                   (xmin, ymax),\n",
    "                                   (xmin, ymin)], fill=(255, 0, 0), width=2)\n",
    "        if display_label:\n",
    "            cleaned_image_drawer.text((xmin + (xmax - xmin) / 2, ymax), label, fill=(255,0,0), font=font)\n",
    "    cleaned_image.save(os.path.join(out_dir, f'{image_name}.jpg'))\n",
    "\n",
    "# Define font\n",
    "font_path = 'C:/Users/iApp/Desktop/projects/object-detection/models/research/object_detection/utils/fonts/angsa.ttf'\n",
    "font = ImageFont.truetype(font_path, 18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw True Positive\n",
    "correct_detection_dir = os.path.join(TEST_RESULT_DIR, 'correct_detection')\n",
    "if not os.path.exists(correct_detection_dir):\n",
    "    os.makedirs(correct_detection_dir)\n",
    "    \n",
    "for image_name, single_image_detections in all_images_detections.items():\n",
    "    image_path = os.path.join(PATH_TO_TEST_IMAGES_DIR, image_name + '.jpg')\n",
    "    detections_to_draw = [detection for detection in single_image_detections if detection['label'] == detection['actual']]\n",
    "    if len(detections_to_draw) > 0:\n",
    "        draw_detection(correct_detection_dir, image_path, detections_to_draw, font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw False Positive\n",
    "wrong_detection_dir = os.path.join(TEST_RESULT_DIR, 'wrong_detection')\n",
    "#if not os.path.exists(wrong_detection_dir):\n",
    "#    os.makedirs(wrong_detection_dir)\n",
    "    \n",
    "for image_name, single_image_detections in all_images_detections.items():\n",
    "    image_path = os.path.join(PATH_TO_TEST_IMAGES_DIR, image_name + '.jpg')\n",
    "    wrong_detections = [detection for detection in single_image_detections if detection['label'] != detection['actual']]\n",
    "    if len(wrong_detections) > 0:\n",
    "        for wrong_detection in wrong_detections:\n",
    "            if wrong_detection['actual'] == 'NA':\n",
    "                out_dir = os.path.join(wrong_detection_dir, 'NA')\n",
    "            else:\n",
    "                out_dir = os.path.join(wrong_detection_dir, str(ord(wrong_detection['actual'])), str(ord(wrong_detection['label'])))\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.makedirs(out_dir)\n",
    "            draw_detection(out_dir, image_path, [wrong_detection], font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw False Negative\n",
    "undetected_dir = os.path.join(TEST_RESULT_DIR, 'undetected')\n",
    "\n",
    "for image_name, single_image_ground_truths in all_images_ground_truths.items():\n",
    "    image_path = os.path.join(PATH_TO_TEST_IMAGES_DIR, image_name + '.jpg')\n",
    "    gts_to_draw = [gt for gt in single_image_ground_truths if gt['predict'] == 'NA']\n",
    "    if len(gts_to_draw) > 0:\n",
    "        for gt in gts_to_draw:\n",
    "            out_dir = os.path.join(undetected_dir, str(ord(gt['label'])))\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.makedirs(out_dir)\n",
    "            draw_detection(out_dir, image_path, [gt], font, display_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw All Detection\n",
    "all_detections_dir = os.path.join(TEST_RESULT_DIR, 'all_detection')\n",
    "if not os.path.exists(all_detections_dir):\n",
    "    os.makedirs(all_detections_dir)\n",
    "for image_name, single_image_detections in all_images_detections.items():\n",
    "    image_path = os.path.join(PATH_TO_TEST_IMAGES_DIR, image_name + '.jpg')\n",
    "    draw_detection(all_detections_dir, image_path, single_image_detections, font) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "object-detection",
   "language": "python",
   "name": "object-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
